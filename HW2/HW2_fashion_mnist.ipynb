{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3908,"status":"ok","timestamp":1644275704495,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"v0hRDnJGY3bH"},"outputs":[],"source":["import os\n","import numpy as np\n","import time\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import math"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1644275704496,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"cNjPazCLXvG8","outputId":"e8b779e7-1d19-4903-be9a-4f38225b8c1d"},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.executing_eagerly()\n","tf.config.list_physical_devices('GPU')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2115,"status":"ok","timestamp":1644275706606,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"usm43hiZX5AN","outputId":"d28ce3fd-b936-4f32-f7ed-eaf45ac8fb02"},"outputs":[{"name":"stdout","output_type":"stream","text":["(55000, 784)\n","(55000,)\n","(10000, 784)\n","(10000,)\n","(5000, 784)\n","(5000,)\n","[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0]\n"]}],"source":["seed=2222\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","size_input = 28*28\n","size_hidden_1 = 256\n","size_hidden_2 = 128\n","size_output = 10\n","batch_size=30\n","lr=0.1\n","dropout_p=0.0\n","L1=0\n","L2=3e-5\n","\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","x_train=tf.reshape(x_train,[x_train.shape[0],-1])\n","x_test=tf.reshape(x_test,[x_test.shape[0],-1])\n","\n","x_train, x_valid = tf.split(  \n","            x_train,\n","            num_or_size_splits=[55000, 5000],\n","            axis=0\n","        )\n","y_train, y_valid = tf.split(\n","            y_train,\n","            num_or_size_splits=[55000, 5000],\n","            axis=0\n","        )\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","print(x_valid.shape)\n","print(y_valid.shape)\n","print(y_test[0:20])"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":203,"status":"ok","timestamp":1644275706804,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"alvV40KYMOrK"},"outputs":[],"source":["class MLP(tf.keras.Model):\n","  def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n","    super(MLP, self).__init__()\n","    \"\"\"\n","    size_input: int, size of input layer\n","    size_hidden: int, size of hidden layer\n","    size_output: int, size of output layer\n","    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n","    \"\"\"\n","\n","    self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n","    size_input, size_hidden_1, size_hidden_2, size_output, device\n","\n","    self.initial=tf.keras.initializers.he_normal(seed=seed)\n","    \n","    # Initialize weights between input layer and hidden layer\n","    self.W1 = tf.Variable(self.initial([self.size_input, self.size_hidden_1]))\n","    # Initialize biases for hidden layer\n","    self.b1 = tf.Variable(self.initial([1, self.size_hidden_1]))\n","     # Initialize weights between hidden layer and output layer\n","    self.W2 = tf.Variable(self.initial([self.size_hidden_1, self.size_hidden_2]))\n","    # Initialize biases for hidden layer\n","    self.b2 = tf.Variable(self.initial([1, self.size_hidden_2]))\n","     # Initialize weights between hidden layer and output layer\n","    self.W3 = tf.Variable(self.initial([self.size_hidden_2, self.size_output]))\n","    # Initialize biases for output layer\n","    self.b3 = tf.Variable(self.initial([1, self.size_output]))\n","    \n","    # Define variables to be updated during backpropagation\n","    self.MLP_variables = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n","\n","    self.loss_object =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    self.reg_12=tf.keras.regularizers.L1L2(l1=L1, l2=L2)\n","    \n","  def forward(self, training, X):\n","    \"\"\"\n","    forward pass\n","    X: Tensor, inputs\n","    \"\"\"\n","    if training==1:\n","      if self.device is not None:\n","        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n","          self.y = self.compute_output(X)\n","      else:\n","        self.y = self.compute_output(X)\n","    elif training==0:\n","      if self.device is not None:\n","        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n","          self.y = self.compute_output_test(X)\n","      else:\n","        self.y = self.compute_output_test(X)\n","      \n","    return self.y\n","  \n","  def loss(self, y_pred, y_true):\n","    '''\n","    y_pred - Tensor of shape (batch_size, size_output)\n","    y_true - Tensor of shape (batch_size, size_output)\n","    '''\n","    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n","    return self.loss_object(y_true, y_pred)+self.reg_12(self.W1)+self.reg_12(self.W2)\n","\n","  def loss2(self, y_pred, y_true):\n","    '''\n","    y_pred - Tensor of shape (batch_size, size_output)\n","    y_true - Tensor of shape (batch_size, size_output)\n","    '''\n","    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n","    return self.loss_object(y_true, y_pred)\n","  \n","  def backward(self, X_train, y_train):\n","    \"\"\"\n","    backward pass\n","    \"\"\"\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n","    #optimizer = tf.keras.optimizers.Adam()\n","    with tf.GradientTape() as tape:\n","      predicted = self.forward(1,X_train)\n","      current_loss = self.loss(predicted, y_train)\n","    grads = tape.gradient(current_loss, self.MLP_variables)\n","    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n","\n","        \n","  def compute_output(self, X):\n","    \"\"\"\n","    Custom method to obtain output tensor during forward pass\n","    \"\"\"\n","    # Cast X to float32\n","    X_tf = tf.cast(X, dtype=tf.float32)\n","    #Remember to normalize your dataset before moving forward\n","    # Compute values in hidden layer\n","    what = tf.matmul(X_tf, self.W1) + self.b1\n","    hhat = tf.nn.relu(what)\n","    hhat = tf.nn.dropout(hhat, rate = dropout_p, seed = seed)\n","    what_1 = tf.matmul(hhat, self.W2) + self.b2\n","    hhat_1 = tf.nn.relu(what_1)\n","    hhat_1 = tf.nn.dropout(hhat_1, rate = dropout_p, seed = seed)\n","    # Compute output\n","    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n","    #output= tf.nn.softmax(what_2)\n","    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n","    #Second add tf.Softmax(output) and then return this variable\n","    return what_2\n","\n","  def compute_output_test(self, X):\n","    \"\"\"\n","    Custom method to obtain output tensor during forward pass\n","    \"\"\"\n","    # Cast X to float32\n","    X_tf = tf.cast(X, dtype=tf.float32)\n","    #Remember to normalize your dataset before moving forward\n","    # Compute values in hidden layer\n","    what = tf.matmul(X_tf, self.W1) + self.b1\n","    hhat = tf.nn.relu(what)\n","    what_1 = tf.matmul(hhat, self.W2) + self.b2\n","    hhat_1 = tf.nn.relu(what_1)\n","    # Compute output\n","    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n","    #output= tf.nn.softmax(what_2)\n","    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n","    #Second add tf.Softmax(output) and then return this variable\n","    return what_2\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644275706804,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"dWR6xLZ2aabo","outputId":"72904fb1-de95-4f5e-ef33-98768efda114"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[[1 1 2 2]]\n","\n"," [[3 2 2 3]]], shape=(2, 1, 4), dtype=int32)\n"]},{"data":{"text/plain":["(2, 2, 2)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["t=np.array([[[1,1],[2,2]],[[3,2],[2,3]]])\n","print(tf.reshape(t,[2,1,-1]))\n","t.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1644275707008,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"IcjnKTHNPvGC"},"outputs":[],"source":["# Set number of epochs\n","NUM_EPOCHS = 20"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64000,"status":"ok","timestamp":1644276488034,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"gBaOygw_Px7Z","outputId":"9ef09b6a-7e1c-4c30-ec3d-688db1aa3dee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Val Loss: 3.3782835006713867, Val Accuracy: 10.760000228881836\n","Epoch 1, Loss: 0.5836119651794434, Accuracy: 82.02000427246094, Val loss: 0.587325394153595, Val Accuracy: 77.56000518798828\n","Epoch 2, Loss: 0.41816744208335876, Accuracy: 86.66363525390625, Val loss: 0.442064493894577, Val Accuracy: 84.18000030517578\n","Epoch 3, Loss: 0.38377049565315247, Accuracy: 88.0345458984375, Val loss: 0.47013506293296814, Val Accuracy: 83.08000183105469\n","Epoch 4, Loss: 0.36135172843933105, Accuracy: 88.80545806884766, Val loss: 0.39085689187049866, Val Accuracy: 85.79999542236328\n","Epoch 5, Loss: 0.344319224357605, Accuracy: 89.24909210205078, Val loss: 0.3883298337459564, Val Accuracy: 85.47999572753906\n","Epoch 6, Loss: 0.33124828338623047, Accuracy: 89.73636627197266, Val loss: 0.3974660634994507, Val Accuracy: 85.5199966430664\n","Epoch 7, Loss: 0.32009562849998474, Accuracy: 90.11272430419922, Val loss: 0.3855273723602295, Val Accuracy: 85.68000030517578\n","Epoch 8, Loss: 0.30936217308044434, Accuracy: 90.4654541015625, Val loss: 0.36710548400878906, Val Accuracy: 86.94000244140625\n","Epoch 9, Loss: 0.3000563085079193, Accuracy: 90.78909301757812, Val loss: 0.3661675453186035, Val Accuracy: 86.73999786376953\n","Epoch 10, Loss: 0.29039236903190613, Accuracy: 91.13272857666016, Val loss: 0.36634546518325806, Val Accuracy: 86.63999938964844\n","Epoch 11, Loss: 0.2829116880893707, Accuracy: 91.46726989746094, Val loss: 0.4107189178466797, Val Accuracy: 84.45999908447266\n","Epoch 12, Loss: 0.2752062678337097, Accuracy: 91.67090606689453, Val loss: 0.346295028924942, Val Accuracy: 87.22000122070312\n","Epoch 13, Loss: 0.26725465059280396, Accuracy: 92.05818176269531, Val loss: 0.3471691906452179, Val Accuracy: 87.18000030517578\n"]}],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n","\n","train_loss.reset_states()\n","train_accuracy.reset_states()\n","val_loss.reset_states()\n","val_accuracy.reset_states()\n","\n","mlp_on_default = MLP(size_input, size_hidden_1, size_hidden_2, size_output)\n","\n","valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size)\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=seed).batch(batch_size)\n","\n","for inputs, outputs in valid_ds:\n","  preds = mlp_on_default.forward(0,inputs)\n","  val_loss(mlp_on_default.loss2(preds,outputs))\n","  val_accuracy(outputs, preds)\n","\n","print(\n","  f'Epoch {0}, '\n","  f'Val Loss: {val_loss.result()}, '\n","  f'Val Accuracy: {val_accuracy.result() * 100}'\n",")\n","\n","time_start = time.time()\n","for epoch in range(NUM_EPOCHS):\n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  val_loss.reset_states()\n","  val_accuracy.reset_states()\n","  \n","  for inputs, outputs in train_ds:\n","    mlp_on_default.backward(inputs, outputs)\n","    preds = mlp_on_default.forward(0,inputs)\n","    train_loss(mlp_on_default.loss(preds,outputs))\n","    train_accuracy(outputs, preds)\n","\n","  for inputs, outputs in valid_ds:\n","    preds = mlp_on_default.forward(0,inputs)\n","    val_loss(mlp_on_default.loss2(preds,outputs))\n","    val_accuracy(outputs, preds)\n","  \n","  print(\n","    f'Epoch {epoch + 1}, '\n","    f'Loss: {train_loss.result()}, '\n","    f'Accuracy: {train_accuracy.result() * 100}, '\n","    f'Val loss: {val_loss.result()}, '\n","    f'Val Accuracy: {val_accuracy.result() * 100}'\n","  )\n","\n","time_taken = time.time() - time_start\n","\n","print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","test_loss.reset_states()\n","test_accuracy.reset_states()\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","for inputs, outputs in test_ds:\n","  preds = mlp_on_default.forward(0,inputs)\n","  test_loss(mlp_on_default.loss2(preds,outputs))\n","  test_accuracy(outputs, preds)\n","\n","print(\n","  f'Epoch {0}, '\n","  f'Test Loss: {test_loss.result()}, '\n","  f'Test Accuracy: {test_accuracy.result() * 100}'\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1644276488296,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"zjEJImTJ9sQU"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HW2_fashion_mnist.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
