{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7071,"status":"ok","timestamp":1644275716536,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"v0hRDnJGY3bH"},"outputs":[],"source":["import os\n","import numpy as np\n","import time\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import math"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":220,"status":"ok","timestamp":1644275716737,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"cNjPazCLXvG8","outputId":"a969f3d1-3866-4d2c-8749-1d1eb330a37a"},"outputs":[{"data":{"text/plain":["[]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["tf.executing_eagerly()\n","tf.config.list_physical_devices('GPU')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2885,"status":"ok","timestamp":1644275719620,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"usm43hiZX5AN","outputId":"1054d8e8-1028-40b6-aa36-4cbd47dcb39c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(55000, 784)\n","(55000,)\n","(10000, 784)\n","(10000,)\n","(5000, 784)\n","(5000,)\n","[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0]\n"]}],"source":["seed=2222\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","\n","size_input = 28*28\n","size_hidden_1 = 256\n","size_hidden_2 = 128\n","size_output = 10\n","batch_size=30\n","lr=0.001\n","dropout_p=0.0\n","L1=0\n","L2=3e-5\n","\n","fashion_mnist = tf.keras.datasets.fashion_mnist\n","(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n","x_train, x_test = x_train / 255.0, x_test / 255.0\n","x_train=tf.reshape(x_train,[x_train.shape[0],-1])\n","x_test=tf.reshape(x_test,[x_test.shape[0],-1])\n","\n","x_train, x_valid = tf.split(  \n","            x_train,\n","            num_or_size_splits=[55000, 5000],\n","            axis=0\n","        )\n","y_train, y_valid = tf.split(\n","            y_train,\n","            num_or_size_splits=[55000, 5000],\n","            axis=0\n","        )\n","print(x_train.shape)\n","print(y_train.shape)\n","print(x_test.shape)\n","print(y_test.shape)\n","print(x_valid.shape)\n","print(y_valid.shape)\n","print(y_test[0:20])"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":223,"status":"ok","timestamp":1644275719837,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"alvV40KYMOrK"},"outputs":[],"source":["class MLP(tf.keras.Model):\n","  def __init__(self, size_input, size_hidden_1, size_hidden_2, size_output, device=None):\n","    super(MLP, self).__init__()\n","    \"\"\"\n","    size_input: int, size of input layer\n","    size_hidden: int, size of hidden layer\n","    size_output: int, size of output layer\n","    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n","    \"\"\"\n","\n","    self.size_input, self.size_hidden_1, self.size_hidden_2, self.size_output, self.device =\\\n","    size_input, size_hidden_1, size_hidden_2, size_output, device\n","\n","    self.initial=tf.keras.initializers.he_normal(seed=seed)\n","    \n","    # Initialize weights between input layer and hidden layer\n","    self.W1 = tf.Variable(self.initial([self.size_input, self.size_hidden_1]))\n","    # Initialize biases for hidden layer\n","    self.b1 = tf.Variable(self.initial([1, self.size_hidden_1]))\n","     # Initialize weights between hidden layer and output layer\n","    self.W2 = tf.Variable(self.initial([self.size_hidden_1, self.size_hidden_2]))\n","    # Initialize biases for hidden layer\n","    self.b2 = tf.Variable(self.initial([1, self.size_hidden_2]))\n","     # Initialize weights between hidden layer and output layer\n","    self.W3 = tf.Variable(self.initial([self.size_hidden_2, self.size_output]))\n","    # Initialize biases for output layer\n","    self.b3 = tf.Variable(self.initial([1, self.size_output]))\n","    \n","    # Define variables to be updated during backpropagation\n","    self.MLP_variables = [self.W1, self.b1, self.W2, self.b2, self.W3, self.b3]\n","\n","    self.loss_object =tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n","    self.reg_12=tf.keras.regularizers.L1L2(l1=L1, l2=L2)\n","\n","    self.t=0\n","    self.m=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n","    self.u=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n","    self.v=[tf.zeros_like(self.W1,dtype=tf.float32), tf.zeros_like(self.b1,dtype=tf.float32),tf.zeros_like(self.W2,dtype=tf.float32),tf.zeros_like(self.b2,dtype=tf.float32),tf.zeros_like(self.W3,dtype=tf.float32),tf.zeros_like(self.b3,dtype=tf.float32)]\n","    \n","  def forward(self, training, X):\n","    \"\"\"\n","    forward pass\n","    X: Tensor, inputs\n","    \"\"\"\n","    if training==1:\n","      if self.device is not None:\n","        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n","          self.y = self.compute_output(X)\n","      else:\n","        self.y = self.compute_output(X)\n","    elif training==0:\n","      if self.device is not None:\n","        with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n","          self.y = self.compute_output_test(X)\n","      else:\n","        self.y = self.compute_output_test(X)\n","      \n","    return self.y\n","  \n","  def loss(self, y_pred, y_true):\n","    '''\n","    y_pred - Tensor of shape (batch_size, size_output)\n","    y_true - Tensor of shape (batch_size, size_output)\n","    '''\n","    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n","    return self.loss_object(y_true, y_pred)+self.reg_12(self.W1)+self.reg_12(self.W2)\n","\n","  def loss2(self, y_pred, y_true):\n","    '''\n","    y_pred - Tensor of shape (batch_size, size_output)\n","    y_true - Tensor of shape (batch_size, size_output)\n","    '''\n","    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n","    return self.loss_object(y_true, y_pred)\n","  \n","  def backward(self, X_train, y_train):\n","    \"\"\"\n","    backward pass\n","    \"\"\"\n","\n","    '''\n","    optimizer = tf.keras.optimizers.SGD(learning_rate=lr)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n","    optimizer = tf.keras.optimizers.RMSprop(learning_rate=lr)\n","    with tf.GradientTape() as tape:\n","      predicted = self.forward(1,X_train)\n","      current_loss = self.loss(predicted, y_train)\n","    grads = tape.gradient(current_loss, self.MLP_variables)\n","    optimizer.apply_gradients(zip(grads, self.MLP_variables))\n","    '''\n","\n","    self.t=self.t+1\n","    beta_1=0.9\n","    beta_2=0.999\n","    beta_3=0.999987\n","    ep=0.00000001\n","    \n","    with tf.GradientTape() as tape:\n","      predicted = self.forward(1,X_train)\n","      current_loss = self.loss(predicted, y_train)\n","    grads = tape.gradient(current_loss, self.MLP_variables)\n","\n","    \n","\n","    self.m=[a_i*beta_1+(1-beta_1)*b_i for a_i, b_i in zip(self.m, grads) ]\n","    self.u=[a_i*beta_2+(1-beta_2)*(b_i**2) for a_i, b_i in zip(self.u, grads) ]\n","    self.v=[a_i*beta_3+(1-beta_3)*(b_i**3) for a_i, b_i in zip(self.v, grads) ]\n","\n","    hm=[a_i/(1-(beta_1**self.t)) for a_i in self.m]\n","    hu=[a_i/(1-(beta_2**self.t)) for a_i in self.u]\n","    hv=[a_i/(1-(beta_3**self.t)) for a_i in self.v]\n","    hv_sign=[tf.math.sign(a_i) for a_i in hv] #Here tf.pow can only handle positive number, so I seperate the sign of hv and abs(hv)\n","    hv_abs=[tf.abs(a_i) for a_i in hv]\n","\n","    dws_new=[lr * a_i /(ep + tf.sqrt(b_i)+(d_i*tf.pow(c_i,1.0/3.0)*ep)) for a_i, b_i, c_i, d_i in zip(hm, hu, hv_abs,hv_sign)] \n","\n","    #print(type(dws[0:1]))\n","    Wt = [a_i - b_i for a_i, b_i in zip(self.variables, dws_new)]\n","    #print(type(Wt))\n","\n","    for i in range(len(self.variables)):\n","      self.variables[i].assign(Wt[i])\n","\n","        \n","  def compute_output(self, X):\n","    \"\"\"\n","    Custom method to obtain output tensor during forward pass\n","    \"\"\"\n","    # Cast X to float32\n","    X_tf = tf.cast(X, dtype=tf.float32)\n","    #Remember to normalize your dataset before moving forward\n","    # Compute values in hidden layer\n","    what = tf.matmul(X_tf, self.W1) + self.b1\n","    hhat = tf.nn.relu(what)\n","    hhat = tf.nn.dropout(hhat, rate = dropout_p, seed = seed)\n","    what_1 = tf.matmul(hhat, self.W2) + self.b2\n","    hhat_1 = tf.nn.relu(what_1)\n","    hhat_1 = tf.nn.dropout(hhat_1, rate = dropout_p, seed = seed)\n","    # Compute output\n","    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n","    #output= tf.nn.softmax(what_2)\n","    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n","    #Second add tf.Softmax(output) and then return this variable\n","    return what_2\n","\n","  def compute_output_test(self, X):\n","    \"\"\"\n","    Custom method to obtain output tensor during forward pass\n","    \"\"\"\n","    # Cast X to float32\n","    X_tf = tf.cast(X, dtype=tf.float32)\n","    #Remember to normalize your dataset before moving forward\n","    # Compute values in hidden layer\n","    what = tf.matmul(X_tf, self.W1) + self.b1\n","    hhat = tf.nn.relu(what)\n","    what_1 = tf.matmul(hhat, self.W2) + self.b2\n","    hhat_1 = tf.nn.relu(what_1)\n","    # Compute output\n","    what_2 = tf.matmul(hhat_1, self.W3) + self.b3\n","    #output= tf.nn.softmax(what_2)\n","    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this\n","    #Second add tf.Softmax(output) and then return this variable\n","    return what_2\n","  "]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1644275719839,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"dWR6xLZ2aabo","outputId":"d9581f4a-8569-44aa-ddad-7337a72868a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["tf.Tensor(\n","[[[1 1 2 2]]\n","\n"," [[3 2 2 3]]], shape=(2, 1, 4), dtype=int32)\n"]},{"data":{"text/plain":["(2, 2, 2)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["t=np.array([[[1,1],[2,2]],[[3,2],[2,3]]])\n","print(tf.reshape(t,[2,1,-1]))\n","t.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1644275719839,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"IcjnKTHNPvGC"},"outputs":[],"source":["# Set number of epochs\n","NUM_EPOCHS = 30"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":163409,"status":"ok","timestamp":1644276587571,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"gBaOygw_Px7Z","outputId":"9df56d54-1fc7-4076-cd38-1341730e6c85"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Val Loss: 3.3782835006713867, Val Accuracy: 10.760000228881836\n","Epoch 1, Loss: 0.46383264660835266, Accuracy: 84.05818176269531, Val loss: 0.42780038714408875, Val Accuracy: 84.5\n","Epoch 2, Loss: 0.3572455048561096, Accuracy: 87.55091094970703, Val loss: 0.3608192801475525, Val Accuracy: 86.9000015258789\n","Epoch 3, Loss: 0.3239750564098358, Accuracy: 88.88727569580078, Val loss: 0.33275580406188965, Val Accuracy: 87.69999694824219\n","Epoch 4, Loss: 0.3062148988246918, Accuracy: 89.71273040771484, Val loss: 0.3278510570526123, Val Accuracy: 87.5199966430664\n","Epoch 5, Loss: 0.2930087149143219, Accuracy: 90.17636108398438, Val loss: 0.320684552192688, Val Accuracy: 87.94000244140625\n","Epoch 6, Loss: 0.2836433947086334, Accuracy: 90.62545013427734, Val loss: 0.3272498846054077, Val Accuracy: 88.23999786376953\n","Epoch 7, Loss: 0.27547475695610046, Accuracy: 91.06182098388672, Val loss: 0.3180219829082489, Val Accuracy: 88.54000091552734\n","Epoch 8, Loss: 0.26506808400154114, Accuracy: 91.25999450683594, Val loss: 0.32908686995506287, Val Accuracy: 87.33999633789062\n","Epoch 9, Loss: 0.2572045624256134, Accuracy: 91.71636199951172, Val loss: 0.3288687765598297, Val Accuracy: 88.26000213623047\n","Epoch 10, Loss: 0.25347232818603516, Accuracy: 91.87818145751953, Val loss: 0.33142420649528503, Val Accuracy: 88.22000122070312\n","Epoch 11, Loss: 0.24645721912384033, Accuracy: 92.19999694824219, Val loss: 0.331326961517334, Val Accuracy: 88.31999969482422\n","Epoch 12, Loss: 0.24111008644104004, Accuracy: 92.4490966796875, Val loss: 0.37413913011550903, Val Accuracy: 87.19999694824219\n","Epoch 13, Loss: 0.2379625141620636, Accuracy: 92.6727294921875, Val loss: 0.3215748965740204, Val Accuracy: 88.52000427246094\n","Epoch 14, Loss: 0.2344866842031479, Accuracy: 92.72909545898438, Val loss: 0.3257575035095215, Val Accuracy: 88.76000213623047\n","Epoch 15, Loss: 0.2282402217388153, Accuracy: 93.08545684814453, Val loss: 0.32079362869262695, Val Accuracy: 88.91999816894531\n","Epoch 16, Loss: 0.22605246305465698, Accuracy: 93.14181518554688, Val loss: 0.34769952297210693, Val Accuracy: 88.3800048828125\n","Epoch 17, Loss: 0.22432275116443634, Accuracy: 93.31454467773438, Val loss: 0.35139000415802, Val Accuracy: 88.22000122070312\n","Epoch 18, Loss: 0.22001659870147705, Accuracy: 93.40908813476562, Val loss: 0.332891583442688, Val Accuracy: 88.3800048828125\n","Epoch 19, Loss: 0.2155681997537613, Accuracy: 93.6181869506836, Val loss: 0.3597371280193329, Val Accuracy: 88.26000213623047\n","Epoch 20, Loss: 0.21347790956497192, Accuracy: 93.7381820678711, Val loss: 0.38255903124809265, Val Accuracy: 87.45999908447266\n","Epoch 21, Loss: 0.21234534680843353, Accuracy: 93.75636291503906, Val loss: 0.3804685175418854, Val Accuracy: 87.87999725341797\n","Epoch 22, Loss: 0.2096826136112213, Accuracy: 94.0199966430664, Val loss: 0.3386695086956024, Val Accuracy: 88.86000061035156\n","Epoch 23, Loss: 0.2056231051683426, Accuracy: 94.13636016845703, Val loss: 0.37887445092201233, Val Accuracy: 87.68000030517578\n","Epoch 24, Loss: 0.20388974249362946, Accuracy: 94.12908935546875, Val loss: 0.3626689016819, Val Accuracy: 87.81999969482422\n","Epoch 25, Loss: 0.20344378054141998, Accuracy: 94.29090881347656, Val loss: 0.3598845601081848, Val Accuracy: 88.26000213623047\n","Epoch 26, Loss: 0.19980166852474213, Accuracy: 94.2963638305664, Val loss: 0.358197957277298, Val Accuracy: 88.34000396728516\n","Epoch 27, Loss: 0.19621406495571136, Accuracy: 94.50727081298828, Val loss: 0.3778267800807953, Val Accuracy: 88.3800048828125\n","Epoch 28, Loss: 0.19677315652370453, Accuracy: 94.64363098144531, Val loss: 0.3662118911743164, Val Accuracy: 88.62000274658203\n","Epoch 29, Loss: 0.19596251845359802, Accuracy: 94.6890869140625, Val loss: 0.37893640995025635, Val Accuracy: 87.77999877929688\n","Epoch 30, Loss: 0.19312018156051636, Accuracy: 94.8272705078125, Val loss: 0.372335821390152, Val Accuracy: 89.26000213623047\n","\n","Total time taken (in seconds): 2153.97\n"]}],"source":["train_loss = tf.keras.metrics.Mean(name='train_loss')\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n","\n","val_loss = tf.keras.metrics.Mean(name='val_loss')\n","val_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='val_accuracy')\n","\n","train_loss.reset_states()\n","train_accuracy.reset_states()\n","val_loss.reset_states()\n","val_accuracy.reset_states()\n","\n","mlp_on_default = MLP(size_input, size_hidden_1, size_hidden_2, size_output)\n","\n","valid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size)\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=seed).batch(batch_size)\n","\n","for inputs, outputs in valid_ds:\n","  preds = mlp_on_default.forward(0,inputs)\n","  val_loss(mlp_on_default.loss2(preds,outputs))\n","  val_accuracy(outputs, preds)\n","\n","print(\n","  f'Epoch {0}, '\n","  f'Val Loss: {val_loss.result()}, '\n","  f'Val Accuracy: {val_accuracy.result() * 100}'\n",")\n","\n","time_start = time.time()\n","for epoch in range(NUM_EPOCHS):\n","  train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000, seed=epoch*seed).batch(batch_size) \n","  \n","  train_loss.reset_states()\n","  train_accuracy.reset_states()\n","  val_loss.reset_states()\n","  val_accuracy.reset_states()\n","  \n","  for inputs, outputs in train_ds:\n","    mlp_on_default.backward(inputs, outputs)\n","    preds = mlp_on_default.forward(0,inputs)\n","    train_loss(mlp_on_default.loss(preds,outputs))\n","    train_accuracy(outputs, preds)\n","\n","  for inputs, outputs in valid_ds:\n","    preds = mlp_on_default.forward(0,inputs)\n","    val_loss(mlp_on_default.loss2(preds,outputs))\n","    val_accuracy(outputs, preds)\n","  \n","  print(\n","    f'Epoch {epoch + 1}, '\n","    f'Loss: {train_loss.result()}, '\n","    f'Accuracy: {train_accuracy.result() * 100}, '\n","    f'Val loss: {val_loss.result()}, '\n","    f'Val Accuracy: {val_accuracy.result() * 100}'\n","  )\n","\n","time_taken = time.time() - time_start\n","\n","print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0, Test Loss: 0.37019082903862, Test Accuracy: 88.91000366210938\n"]}],"source":["test_loss = tf.keras.metrics.Mean(name='test_loss')\n","test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n","\n","test_loss.reset_states()\n","test_accuracy.reset_states()\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n","\n","for inputs, outputs in test_ds:\n","  preds = mlp_on_default.forward(0,inputs)\n","  test_loss(mlp_on_default.loss2(preds,outputs))\n","  test_accuracy(outputs, preds)\n","\n","print(\n","  f'Epoch {0}, '\n","  f'Test Loss: {test_loss.result()}, '\n","  f'Test Accuracy: {test_accuracy.result() * 100}'\n",")"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":365},"executionInfo":{"elapsed":337,"status":"ok","timestamp":1644276587905,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"hB5YOqnjDA8o","outputId":"7abc79d4-b779-46bc-9baa-5e7f8ed06b7b"},"outputs":[{"data":{"text/plain":["'\\nplt.figure(figsize=(13,5))\\nplt.grid(linestyle = \"--\") \\nax = plt.gca()\\n\\nx_c = np.array([0.0,0.05,0.1,0.2,0.3]) \\nC=np.array([0.07,0.0695,0.069,0.061,0.065])  \\n\\nplt.plot(x_c,C,\\'ro-\\',linewidth=1.5)\\n#ax.fill_between(x_c, (C1), (C2), color=\\'r\\', alpha=.1)\\nplt.xticks(fontsize=16,fontweight=\\'bold\\') \\nplt.yticks(fontsize=16,fontweight=\\'bold\\')\\nplt.xlabel(\"Dropout p\",fontsize=18,fontweight=\\'bold\\')\\nplt.ylabel(\"Test loss\",fontsize=18,fontweight=\\'bold\\')\\n#plt.xlim(-0.5,3.5)\\n#plt.ylim(0.5,1.5)\\n#x_major_locator=plt.MultipleLocator(1)\\n#y_major_locator=plt.MultipleLocator(0.2)\\n#ax.xaxis.set_major_locator(x_major_locator)\\n#ax.yaxis.set_major_locator(y_major_locator)\\nplt.legend(loc=0, numpoints=1)\\nleg = plt.gca().get_legend()\\nltext = leg.get_texts()\\nplt.setp(ltext, fontsize=18,fontweight=\\'bold\\') \\nplt.show()\\n'"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","plt.figure(figsize=(13,5))\n","plt.grid(linestyle = \"--\") \n","ax = plt.gca()\n","\n","x_c = np.array([0.0,0.05,0.1,0.2,0.3]) \n","C=np.array([0.07,0.0695,0.069,0.061,0.065])  \n","\n","plt.plot(x_c,C,'ro-',linewidth=1.5)\n","#ax.fill_between(x_c, (C1), (C2), color='r', alpha=.1)\n","plt.xticks(fontsize=16,fontweight='bold') \n","plt.yticks(fontsize=16,fontweight='bold')\n","plt.xlabel(\"Dropout p\",fontsize=18,fontweight='bold')\n","plt.ylabel(\"Test loss\",fontsize=18,fontweight='bold')\n","#plt.xlim(-0.5,3.5)\n","#plt.ylim(0.5,1.5)\n","#x_major_locator=plt.MultipleLocator(1)\n","#y_major_locator=plt.MultipleLocator(0.2)\n","#ax.xaxis.set_major_locator(x_major_locator)\n","#ax.yaxis.set_major_locator(y_major_locator)\n","plt.legend(loc=0, numpoints=1)\n","leg = plt.gca().get_legend()\n","ltext = leg.get_texts()\n","plt.setp(ltext, fontsize=18,fontweight='bold') \n","plt.show()\n","'''"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1644276587906,"user":{"displayName":"Siyuan Xu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02533286040223950786"},"user_tz":300},"id":"zjEJImTJ9sQU"},"outputs":[{"data":{"text/plain":["'\\nplt.figure(figsize=(13,5))\\nplt.grid(linestyle = \"--\") \\nax = plt.gca()\\n\\nx_c = np.array([0.0,0.00001,0.00003,0.0001,0.0003]) \\nC=np.array([0.07,0.069,0.064,0.067,0.069])  \\n\\nplt.plot(x_c,C,\\'ro-\\',linewidth=1.5)\\n#ax.fill_between(x_c, (C1), (C2), color=\\'r\\', alpha=.1)\\nplt.xticks(fontsize=16,fontweight=\\'bold\\') \\nplt.yticks(fontsize=16,fontweight=\\'bold\\')\\nplt.xlabel(\"Coefficients of L2\",fontsize=18,fontweight=\\'bold\\')\\nplt.ylabel(\"Test loss\",fontsize=18,fontweight=\\'bold\\')\\n#plt.xlim(-0.5,3.5)\\n#plt.ylim(0.5,1.5)\\n#x_major_locator=plt.MultipleLocator(1)\\n#y_major_locator=plt.MultipleLocator(0.2)\\n#ax.xaxis.set_major_locator(x_major_locator)\\n#ax.yaxis.set_major_locator(y_major_locator)\\nplt.legend(loc=0, numpoints=1)\\nleg = plt.gca().get_legend()\\nltext = leg.get_texts()\\nplt.setp(ltext, fontsize=18,fontweight=\\'bold\\') \\nplt.show()\\n'"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","plt.figure(figsize=(13,5))\n","plt.grid(linestyle = \"--\") \n","ax = plt.gca()\n","\n","x_c = np.array([0.0,0.00001,0.00003,0.0001,0.0003]) \n","C=np.array([0.07,0.069,0.064,0.067,0.069])  \n","\n","plt.plot(x_c,C,'ro-',linewidth=1.5)\n","#ax.fill_between(x_c, (C1), (C2), color='r', alpha=.1)\n","plt.xticks(fontsize=16,fontweight='bold') \n","plt.yticks(fontsize=16,fontweight='bold')\n","plt.xlabel(\"Coefficients of L2\",fontsize=18,fontweight='bold')\n","plt.ylabel(\"Test loss\",fontsize=18,fontweight='bold')\n","#plt.xlim(-0.5,3.5)\n","#plt.ylim(0.5,1.5)\n","#x_major_locator=plt.MultipleLocator(1)\n","#y_major_locator=plt.MultipleLocator(0.2)\n","#ax.xaxis.set_major_locator(x_major_locator)\n","#ax.yaxis.set_major_locator(y_major_locator)\n","plt.legend(loc=0, numpoints=1)\n","leg = plt.gca().get_legend()\n","ltext = leg.get_texts()\n","plt.setp(ltext, fontsize=18,fontweight='bold') \n","plt.show()\n","'''"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"HW2.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
